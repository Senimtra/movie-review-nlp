{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import textwrap\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/imdb_train.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "\n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying duplicate reviews\n",
    "\n",
    "duplicates = len(data[data.duplicated(subset = 'review')])\n",
    "\n",
    "print('Duplicates found:', duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample review\n",
    "\n",
    "sample_review = textwrap.fill(data.sample().iloc[0, 0], width = 100)\n",
    "\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review word count + average review length\n",
    "\n",
    "word_count_range = [len(text.split()) for text in data.review]\n",
    "average_word_count = sum(word_count_range) / len(word_count_range)\n",
    "\n",
    "print(f\"Review length (words): {min(word_count_range)} - {max(word_count_range)} words\")\n",
    "print(f\"Average review length (words): {average_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting review word counts\n",
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.hist(word_count_range, bins = 150, edgecolor = 'black', alpha = 0.7)\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Review Word Counts')\n",
    "plt.grid(axis = 'y', linestyle = '--', alpha = 0.6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring BERT token length distribution BEFORE data cleaning\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "over_512 = 0\n",
    "within_512 = 0\n",
    "\n",
    "token_count_list = []\n",
    "\n",
    "for review in data['review']:\n",
    "    token_count = len(tokenizer.tokenize(review)) + 2\n",
    "    token_count_list.append(token_count)\n",
    "    if token_count > 512:\n",
    "        over_512 += 1\n",
    "    else:\n",
    "        within_512 +=1\n",
    "\n",
    "print('Number of reviews with more than 512 tokens:', over_512)\n",
    "print('Number of reviews with 512 tokens or fewer:', within_512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting BERT token length distribution BEFORE data cleaning\n",
    "\n",
    "plt.figure(figsize = (10, 3))\n",
    "plt.hist(token_count_list, bins = 150, edgecolor = 'black', color = 'orange', alpha = 0.7)\n",
    "plt.axvline(x = 512, color = 'red', linestyle = '--', linewidth = 1.5) \n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Token Lengths BEFORE data cleaning')\n",
    "plt.grid(axis = 'y', linestyle = '--', alpha = 0.6)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
